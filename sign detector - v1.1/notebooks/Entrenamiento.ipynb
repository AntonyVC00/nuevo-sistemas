{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceec82f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.22.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\usuario\\appdata\\roaming\\python\\python310\\site-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (5.29.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd9671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.18.0)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnx) (2.1.3)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnx) (5.29.4)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\users\\usuario\\appdata\\roaming\\python\\python310\\site-packages (from onnx) (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ca1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\usuario\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22372a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'nothing': 27, 'space': 28}\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "dataset = datasets.ImageFolder('/Users/Usuario/Desktop/sign detector - 1.1/dataset2/test')\n",
    "print(dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82248444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2600/2600] Procesando: z_7_rotate_3.jpeg | Tiempo transcurrido: 648.5s | Tiempo restante estimado: 0.0sssss\n",
      "\n",
      "Evaluación completada.\n",
      "\n",
      "Total de imágenes evaluadas: 2600\n",
      "Precisión (accuracy): 97.92%\n",
      "Pérdida promedio (loss): 0.0694\n"
     ]
    }
   ],
   "source": [
    "#PARA TESTEAR TODA LA CARPETA DE IMAGENES\n",
    "\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Ruta del modelo ONNX\n",
    "model_path = \"/users/Usuario/Desktop/sign detector - 1.1/onnx_model/stacked_model_mobilenet_resnet.onnx\"\n",
    "session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "# Transformación igual a la del entrenamiento\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Clases en orden\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
    "               'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'borrar', 'nada', 'espacio']\n",
    "class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_t = transform(img)\n",
    "    return img_t.numpy()[np.newaxis, ...]\n",
    "\n",
    "# Ruta a la carpeta de test\n",
    "test_dir = (\"/users/Usuario/Desktop/sign detector - 1.1/dataset2/test\")\n",
    "\n",
    "# Obtener todas las rutas de imágenes y etiquetas\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for class_name in os.listdir(test_dir):\n",
    "    class_path = os.path.join(test_dir, class_name)\n",
    "    if not os.path.isdir(class_path) or class_name not in class_to_idx:\n",
    "        continue\n",
    "    label_idx = class_to_idx[class_name]\n",
    "\n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        image_paths.append(img_path)\n",
    "        labels.append(label_idx)\n",
    "\n",
    "total_images = len(image_paths)\n",
    "correct = 0\n",
    "losses = []\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Procesar cada imagen\n",
    "for i, (img_path, label_idx) in enumerate(zip(image_paths, labels), 1):\n",
    "    try:\n",
    "        input_image = preprocess_image(img_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con imagen {img_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    outputs = session.run(None, {input_name: input_image})\n",
    "    logits = outputs[0]\n",
    "    pred = np.argmax(logits, axis=1)[0]\n",
    "\n",
    "    if pred == label_idx:\n",
    "        correct += 1\n",
    "\n",
    "    logits_tensor = torch.tensor(logits)\n",
    "    target_tensor = torch.tensor([label_idx])\n",
    "    loss = F.cross_entropy(logits_tensor, target_tensor)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Progreso y tiempo estimado\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_time_per_image = elapsed / i\n",
    "    remaining_time = avg_time_per_image * (total_images - i)\n",
    "\n",
    "    print(f\"[{i}/{total_images}] Procesando: {os.path.basename(img_path)} | \"\n",
    "          f\"Tiempo transcurrido: {elapsed:.1f}s | \"\n",
    "          f\"Tiempo restante estimado: {remaining_time:.1f}s\", end='\\r')\n",
    "\n",
    "print(\"\\n\\nEvaluación completada.\\n\")\n",
    "\n",
    "# Resultados finales\n",
    "accuracy = correct / total_images * 100\n",
    "avg_loss = np.mean(losses)\n",
    "\n",
    "print(f\"Total de imágenes evaluadas: {total_images}\")\n",
    "print(f\"Precisión (accuracy): {accuracy:.2f}%\")\n",
    "print(f\"Pérdida promedio (loss): {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e567c014",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 125\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(stacked_model, test_loader, criterion, device)\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 89\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     87\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     88\u001b[0m running_loss, running_corrects, total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     90\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     91\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 29\n",
    "dataset_path = 'C:/users/Usuario/Desktop/sign detector - 1.1/dataset2'\n",
    "\n",
    "# === Definición modelos base ===\n",
    "\n",
    "# ResNet50\n",
    "resnet = models.resnet50(weights='IMAGENET1K_V1')\n",
    "in_features_resnet = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(in_features_resnet, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, num_classes)\n",
    ")\n",
    "#resnet.load_state_dict(torch.load(\"resnet_best_model.pth\", map_location=device))\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# MobileNetV2\n",
    "mobilenet = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "in_features_mobilenet = mobilenet.classifier[1].in_features\n",
    "mobilenet.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features_mobilenet, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, num_classes)\n",
    ")\n",
    "#mobilenet.load_state_dict(torch.load(\"mobilenetv2_best_model.pth\", map_location=device))\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# === Modelo Stacking ===\n",
    "class StackedModel(nn.Module):\n",
    "    def __init__(self, model1, model2, num_classes=29):\n",
    "        super(StackedModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_classes * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.model1(x)\n",
    "        out2 = self.model2(x)\n",
    "        combined = torch.cat((out1, out2), dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "stacked_model = StackedModel(resnet, mobilenet, num_classes=num_classes).to(device)\n",
    "\n",
    "# === Congelar modelos base para entrenar solo la capa combinadora ===\n",
    "for param in stacked_model.model1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in stacked_model.model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# === Transforms, datasets y loaders ===\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(dataset_path, 'train'), transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = ImageFolder(os.path.join(dataset_path, 'test'), transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# === Funciones de entrenamiento y evaluación ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, stacked_model.parameters()), lr=1e-4)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, running_corrects, total_samples = 0, 0, 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == targets).item()\n",
    "        total_samples += inputs.size(0)\n",
    "    return running_loss / total_samples, running_corrects / total_samples\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    acc = total_correct / total_samples\n",
    "    return avg_loss, acc\n",
    "\n",
    "# === Entrenamiento ===\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(stacked_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(stacked_model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# === Evaluación final ===\n",
    "test_loss, test_acc = evaluate(stacked_model, test_loader, criterion, device)\n",
    "print(f\"\\n🔗 Test Acc Stacked Model: {test_acc:.4f} ({test_acc*100:.2f}%) | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# === Exportar a ONNX ===\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "torch.onnx.export(\n",
    "    stacked_model,\n",
    "    dummy_input,\n",
    "    \"stacked_model_mobilenet_resnet.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "print(\"✅ Modelo combinado exportado a stacked_model_mobilenet_resnet.onnx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
